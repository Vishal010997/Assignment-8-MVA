---
title: "MVA Assingnment 8"
output: html_document
date: "2024-04-14"
---

Loading Data:

```{r}
library(readxl)
social_media <- read_excel("C:/Users/Vishal/Downloads/MVA_CLASS_COMBINE.xlsx")
social_media
str(social_media)
social_media_cleaned <- social_media[,-1]

```



changing column names:


```{R}

#changing column names
change_cols_index <- c(2,4,6,8,10,12,14,16,17,18,19,20,21,22,23,24)
change_cols_name <- c("Instagram_Time", "Linkedin_Time", "Snapchat_Time", "Twitter_Time", "Whatsapp_Time", "Youtube_Time", "OTT_Time", "Reddit_Time", "Application Type", "Interview_call_received", "Networking", "Learning", "Mood_Productivity", "Morning_tireness", "Sleep_trouble", "Weekly_Feelings")
colnames(social_media_cleaned)[change_cols_index] <- change_cols_name



social_media_cleaned


```



Cleaning Data:

Cleaning Null values

```{R}
# Convert "NA", "N/A", "n/a", "na", "N.A", "n.a" to 0
social_media_cleaned[social_media_cleaned == "NA" | social_media_cleaned == "N/A" | social_media_cleaned == "na" | social_media_cleaned == "n/a" | social_media_cleaned == "N.A" | social_media_cleaned == "n.a" | social_media_cleaned == "0" | social_media_cleaned == ""] <- NA
social_media_cleaned

```


Null values converted to 0

```{R}
social_media_cleaned[is.na(social_media_cleaned)] <- '0'
social_media_cleaned
```


Keeping relevant columns only:
All time columns + label to predict ("How did you feel enitre week") + Application type

```{R}
# Define a function to convert time strings to decimal hours
convert_to_decimal_hours <- function(time_string) {
# Check if NA values are present
if (any(is.na(time_string))) {
         return(rep(NA, length(time_string)))  # Return NA for NA values
     }
     
# Define a function to convert HH:MM format to decimal hours
     hhmm_to_decimal <- function(hhmm) {
         parts <- as.numeric(strsplit(hhmm, ":")[[1]])  # Split into hours and minutes
         hours <- parts[1]
         minutes <- ifelse(length(parts) > 1, parts[2], 0)  # Handle missing minutes
         total_hours <- hours + minutes / 60
         return(total_hours)
     }
     
# Convert time strings to decimal hours
decimal_hours <- sapply(time_string, function(x) {
         if (grepl("^\\d+:\\d+$", x)) {
             return(hhmm_to_decimal(x))  # Convert HH:MM format
         } else if (grepl("^\\d+\\.\\d+$", x)) {
             return(as.numeric(x))  # Convert decimal format
         } else if (grepl("^\\d+$", x)) {
             return(as.numeric(x))  # Convert whole numbers
         } else {
             return(NA)  # Return NA for other cases
         }
     })
     
     return(decimal_hours)
}

time_columns <- c("Instagram_Time", "Linkedin_Time", "Snapchat_Time", "Twitter_Time", "Whatsapp_Time", "Youtube_Time", "OTT_Time", "Reddit_Time") 
# Apply the conversion function to all time columns
social_media_cleaned[time_columns] <- lapply(social_media_cleaned[time_columns], convert_to_decimal_hours)
 
# Verify the result
str(social_media_cleaned)

#Dropping the name columns
social_media_cleaned <- social_media_cleaned[, -c(1, 3, 5, 7, 9, 11, 13, 15)] 
social_media_cleaned
```

Data Preporcessing:

Replace mean value with null values for data preprocessing

```{R}
# Loop through each column in time_columns
social_media_cleaned[time_columns] <- lapply(social_media_cleaned[time_columns], function(x) {
  # Calculate mean of the column excluding NA values
  mean_value <- mean(x, na.rm = TRUE)
  # Replace NA values with the mean
  x[is.na(x)] <- mean_value
  return(x)
})

# Print the updated data frame
print(social_media_cleaned)
```



```{R}
# Find columns with "_Time"
time_columns <- grep("_Time$", names(social_media_cleaned), value = TRUE)
time_columns
# Define additional columns to keep
additional_columns <- c("Weekly_Feelings", "Application Type")

# Combine time columns and additional columns to keep
columns_to_keep <- c(time_columns, additional_columns)

# Select columns to keep from the dataframe
social_media_subset <- social_media_cleaned[columns_to_keep]
```

Data preprocessing:

Finding mean of numeric training variables aggregated to Application Type

setting application type as row names for multiple regression

```{r}

# Calculate the mean of numeric attributes grouped by Application Type
mean_by_application <- aggregate(. ~ `Application Type`, data = social_media_subset, FUN = mean)

# Set row names to "Application Type"
rownames(mean_by_application) <- mean_by_application$`Application Type`

mean_by_application <- mean_by_application[, -1]

mean_by_application
```




```{R}
attach(mean_by_application)
str(mean_by_application)
# Convert mean_by_application to a data frame
mean_by_application <- data.frame(mean_by_application)
mean_by_application
```


1) Model Development (2 points)


Development:


training attributes: Instagram_Time + Whatsapp_Time  + Linkedin_Time

Prediction label: Weekly_Feelings 


Inference: 

Instagram_Time Coefficient: The negative coefficient (-1.4322) suggests that as Instagram_Time increases, Weekly_Feelings tend to decrease. This could imply that spending more time on Instagram may have a negative impact on the feelings reported weekly.

Whatsapp_Time Coefficient: Although the coefficient (0.3268) indicates a positive relationship between Whatsapp_Time and Weekly_Feelings, it is not statistically significant. Therefore, we cannot confidently conclude that there is a meaningful association between these variables.

Linkedin_Time Coefficient: The positive coefficient (0.8608) indicates that as Linkedin_Time increases, Weekly_Feelings tend to increase as well. This suggests that spending more time on LinkedIn may have a positive impact on the reported weekly feelings.

Overall Model Significance: The F-statistic (3.393) and its associated p-value (0.2359) suggest that the overall model might not be statistically significant at the conventional significance level of 0.05. This implies that the combination of predictor variables (Instagram_Time, Whatsapp_Time, and Linkedin_Time) may not adequately explain the variability in Weekly_Feelings.

Adjusted R-squared: The adjusted R-squared value (0.5895) indicates that approximately 58.95% of the variability in Weekly_Feelings can be explained by the predictor variables in the model. While this is a relatively moderate level of explanatory power, it suggests that there may be other factors not accounted for in the model that influence weekly feelings.

In summary, the analysis suggests that spending more time on Instagram may be associated with lower weekly feelings, while spending more time on LinkedIn may be associated with higher weekly feelings. However, the overall model's significance is not strong, indicating that additional factors may need to be considered to better understand and predict weekly feelings.


```{R}
# Performing multiple regression on mtcars dataset
# Assuming mean_by_application is a data frame
# Fit linear regression model
# Fit linear regression model
fit <- lm(Weekly_Feelings ~ Instagram_Time + Whatsapp_Time  + Linkedin_Time  , data = mean_by_application)

#show the results
summary(fit)
fit
```


2) Model Acceptance (2 points)

Inference on selected training X-attributes:

Intercept (3.1618): The intercept represents the expected value of Weekly_Feelings when all predictor variables (Instagram_Time, Whatsapp_Time, and Linkedin_Time) are zero. It indicates the baseline level of Weekly_Feelings.

Instagram_Time (-1.4322): For every unit increase in Instagram_Time, Weekly_Feelings decreases by approximately 1.4322 units. This suggests that spending more time on Instagram may have a negative impact on Weekly_Feelings.

Whatsapp_Time (0.3268): The coefficient for Whatsapp_Time is positive, indicating that there is a positive relationship between Whatsapp_Time and Weekly_Feelings. However, the coefficient is relatively small, suggesting a weaker influence compared to other variables.

Linkedin_Time (0.8608): For every unit increase in Linkedin_Time, Weekly_Feelings increases by approximately 0.8608 units. This implies that spending more time on LinkedIn may have a positive impact on Weekly_Feelings.



```{R}

#Summary has three sections. Section1: How well does the model fit the data (before Coefficients). Section2: Is the hypothesis supported? (until sifnif codes). Section3: How well does data fit the model (again).
# Useful Helper Functions
coefficients(fit)


```
Inference:

[1]Linked_in time and youtube lime are linearly correlated

[2] Instagram and whatsapp seem to boe correlated in nature to other attribute hence we can choose these attributes for our model

```{R}
library(GGally)
ggpairs(data=mean_by_application, title="Social Media data")
```
```{r}
confint(fit,level=0.95)
```

4) Prediction (2 points)


Learning (4.1425): The model predicts a Weekly_Feelings value of approximately 4.1425 for observations related to learning activities.

Netflix (3.1074): For activities related to Netflix, the predicted Weekly_Feelings value is around 3.1074.

No Social Media (2.8226): Instances where no social media activities are reported correspond to a predicted Weekly_Feelings value of approximately 2.8226.

OTT (3.0607): The model estimates a Weekly_Feelings value of about 3.0607 for activities related to Over-the-Top (OTT) platforms.

Social Media (3.2689): Instances involving social media activities yield a predicted Weekly_Feelings value of approximately 3.2689.

Social Media (3.6312): Another instance of social media activities corresponds to a predicted Weekly_Feelings value of around 3.6312.


```{R}
fitted(fit)
```

3) Residual Analysis (2 points)


The residuals represent the discrepancies between the observed and predicted values of the response variable (Weekly_Feelings).

[1] For observations related to "Learning," "Netflix," and "OTT," the residuals are negative, indicating that the model tends to overestimate their corresponding Weekly_Feelings values.

[2] Conversely, for observations associated with "Social media" and "Social Media," the residuals are positive, suggesting that the model underestimates their Weekly_Feelings values.


These discrepancies might imply that the model does not fully capture the variability in Weekly_Feelings for certain categories or observations. Further investigation and possibly model refinement may be necessary to improve the accuracy of predictions for these cases.


```{r}
residuals(fit)
```


Instagram_Time: The p-value (0.6003) for this predictor is greater than the significance level of 0.05, indicating that Instagram_Time is not a statistically significant predictor of Weekly_Feelings.


Whatsapp_Time: The p-value (0.1384) for this predictor is less than 0.05, suggesting that Whatsapp_Time may have a significant effect on Weekly_Feelings. However, further investigation is needed to confirm its significance.


Linkedin_Time: The p-value (0.1823) for this predictor is also greater than 0.05, indicating that Linkedin_Time is a moderatedly statistically significant predictor of Weekly_Feelings.


```{r}
#Anova Table
anova(fit)
```
The values on the diagonal represent the variance of each coefficient.

For example, the variance of the intercept (Intercept) is approximately 0.09294413.

The off-diagonal values represent the covariance between pairs of coefficients.

For example, the covariance between Instagram_Time and Whatsapp_Time is approximately 0.0973592

```{r}
  vcov(fit)
```

Interpretation:

The correlation between Instagram_Time and Whatsapp_Time is 0.3516115, indicating a moderate positive correlation between these two variables.


The correlation between Instagram_Time and Linkedin_Time is -0.8006927, suggesting a strong negative correlation between them.


The correlation between Whatsapp_Time and Linkedin_Time is -0.5986843, indicating a moderate negative correlation between them.


```{R}
cov2cor(vcov(fit))
```

Residual plot:

[1] Social Media has high value for residual plot

[2] No social media seems to have less residual value

[3] Leaning has high deviation from actual values


Q-Q Residuals:

[1] OTT and social media have positive residuals

[2] Learning has negative deviation from actual


Residual vs leverage plot:

[1] Social media prediction seems to be far from actuals

```{R}
temp <- influence.measures(fit)
temp
plot(fit)

```


```{R}
# Set the options to display numbers without scientific notation
options(scipen = 999)

library(MASS)
sresid <- studres(fit)
hist(sresid, freq=FALSE,
     main="Distribution of Studentized Residuals")
xfit<-seq(min(sresid),max(sresid),length=37)
yfit<-dnorm(xfit)
xfit
yfit

lines(xfit, yfit)
```

5) Model Accuracy (2 points)

Variance Formula:

The variance of the residuals is expected to change with the fitted values of the regression model.
Chisquare:

The Chisquare value, which measures the discrepancy between observed and expected variances, is 0.02945971.
A small Chisquare value suggests good agreement between observed and expected variances.
Degrees of Freedom (Df):

There is one degree of freedom associated with the Chisquare test.
p-value:

The p-value associated with the Chisquare statistic is 0.86372.
With a high p-value, there is insufficient evidence to reject the null hypothesis of constant error variance.
This suggests that the model's residuals exhibit constant variance.
Suggested Power Transformation:

The suggested power transformation is 0.4636738.
However, since the test did not detect evidence of non-constant error variance, applying this transformation may not be necessary.




```{r}
library(car)



#Non-constant Error Variance
# Evaluate homoscedasticity
# non-constant error variance test
ncvTest(fit)
# plot studentized residuals vs. fitted values
spreadLevelPlot(fit)


```


5) Model Accuracy (2 points)


Instagram_Time, Whatsapp_Time, Linkedin_Time:

The values represent the coefficients estimated for each predictor variable in the regression model.


Instagram_Time Whatsapp_Time Linkedin_Time:

These values indicate whether the corresponding coefficients are statistically significant (TRUE) or not (FALSE) based on their p-values.


lag, Autocorrelation, D-W Statistic, p-value:

lag: Indicates the number of lags used in the Durbin-Watson test.

Autocorrelation: Represents the autocorrelation coefficient estimated by the Durbin-Watson test. It measures the strength and direction of the linear relationship between lagged residuals.

D-W Statistic: Denotes the Durbin-Watson test statistic, which assesses the presence of autocorrelation in the residuals. 

It ranges from 0 to 4, with values close to 2 indicating no autocorrelation.


p-value: Indicates the significance level associated with the Durbin-Watson test statistic. A low p-value suggests evidence against the null hypothesis of no autocorrelation.



Based on these values:

For "Instagram_Time" and "Whatsapp_Time," the coefficients are not statistically significant (FALSE), suggesting that they may not have a significant impact on the dependent variable.


For "Linkedin_Time," the coefficient is statistically significant (TRUE), indicating that it likely has a significant impact on the dependent variable.


The Durbin-Watson test suggests a low autocorrelation coefficient (0.200862) and a p-value of 0.056, indicating weak evidence against the null hypothesis of no autocorrelation. This suggests some autocorrelation may be present in the residuals, but it is not significant at the conventional significance level of 0.05.



```{R}
#Multi-collinearity
# Evaluate Collinearity
vif(fit) # variance inflation factors
sqrt(vif(fit)) > 2 # problem?
#Nonlinearity
# component + residual plot
crPlots(fit)

#Non-independence of Errors
# Test for Autocorrelated Errors
durbinWatsonTest(fit)
```

